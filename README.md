# lm-watermark-lab

[![Build Status](https://img.shields.io/github/actions/workflow/status/your-org/lm-watermark-lab/ci.yml?branch=main)](https://github.com/your-org/lm-watermark-lab/actions)
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache--2.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Paper](https://img.shields.io/badge/paper-ACL%202023-red.svg)](https://arxiv.org/abs/2301.10226)

Comprehensive toolkit for watermarking, detecting, and attacking LLM-generated text. Compare multiple watermarking algorithms, evaluate robustness against paraphrasing attacks, and visualize the detectability-quality trade-offs.

## ğŸ¯ Key Features

- **Multi-Algorithm Support**: Implements 10+ watermarking schemes in one framework
- **Attack Suite**: Paraphrasing, truncation, and adversarial attacks on watermarks
- **Quality Metrics**: BLEU, perplexity, and human evaluation proxies
- **Real-time Detection**: Fast watermark detection APIs with confidence scores
- **Forensics Tools**: Trace watermark origins and analyze contamination
- **Visual Analytics**: Interactive dashboards for watermark analysis

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Watermarking Methods](#watermarking-methods)
- [Detection](#detection)
- [Attacks](#attacks)
- [Evaluation](#evaluation)
- [Forensics](#forensics)
- [API Reference](#api-reference)
- [Benchmarks](#benchmarks)
- [Contributing](#contributing)

## ğŸš€ Installation

### From PyPI

```bash
pip install lm-watermark-lab
```

### From Source

```bash
git clone https://github.com/your-org/lm-watermark-lab
cd lm-watermark-lab
pip install -e ".[all]"
```

### Docker Installation

```bash
docker pull your-org/watermark-lab:latest
docker run -it -p 8080:8080 your-org/watermark-lab:latest
```

## âš¡ Quick Start

### Basic Watermarking

```python
from watermark_lab import WatermarkFactory, WatermarkDetector

# Initialize watermarker
watermarker = WatermarkFactory.create(
    method="kirchenbauer",  # or "aaronson", "zhao", "MarkLLM"
    model_name="gpt2-medium",
    key="secret_key_123"
)

# Generate watermarked text
prompt = "The future of AI is"
watermarked_text = watermarker.generate(
    prompt,
    max_length=200,
    temperature=0.7
)

# Detect watermark
detector = WatermarkDetector(watermarker.get_config())
result = detector.detect(watermarked_text)

print(f"Watermark detected: {result.is_watermarked}")
print(f"Confidence: {result.confidence:.2%}")
print(f"P-value: {result.p_value:.4f}")
```

### Comparing Methods

```python
from watermark_lab import WatermarkBenchmark

# Compare multiple watermarking methods
benchmark = WatermarkBenchmark()

methods = ["kirchenbauer", "aaronson", "zhao", "MarkLLM", "unigram"]
results = benchmark.compare(
    methods=methods,
    prompts=load_test_prompts(),
    metrics=["detectability", "quality", "robustness"]
)

# Visualize trade-offs
benchmark.plot_pareto_frontier(
    results,
    x_axis="perplexity_increase",
    y_axis="detection_accuracy",
    save_to="watermark_comparison.png"
)
```

## ğŸ’§ Watermarking Methods

### Supported Algorithms

```python
from watermark_lab.methods import list_available_methods

# List all implemented methods
methods = list_available_methods()
for method in methods:
    print(f"{method.name}: {method.description}")
    print(f"  - Paper: {method.paper_url}")
    print(f"  - Type: {method.watermark_type}")
```

### Kirchenbauer et al. Method

```python
from watermark_lab.methods import KirchenbauerWatermark

watermark = KirchenbauerWatermark(
    model="facebook/opt-1.3b",
    gamma=0.25,  # Greenlist ratio
    delta=2.0,   # Bias strength
    seed=42
)

# Custom generation parameters
output = watermark.generate(
    prompt="Write a story about",
    max_new_tokens=100,
    greenlist_bias=True
)
```

### MarkLLM Integration

```python
from watermark_lab.methods import MarkLLMWatermark

# Use THU-BPM's MarkLLM
watermark = MarkLLMWatermark(
    algorithm="KGW",  # or "SWEET", "XSIR"
    model="llama-2-7b",
    watermark_config={
        "vocab_partition_ratio": 0.5,
        "watermark_strength": 2.0
    }
)

# Generate with watermark
text = watermark.generate_text(prompt)

# Export watermark key
key = watermark.export_key("watermark_key.json")
```

### Custom Watermark Design

```python
from watermark_lab import BaseWatermark

class CustomWatermark(BaseWatermark):
    """Implement your own watermarking scheme."""
    
    def __init__(self, model, secret_key):
        super().__init__(model)
        self.key = self.hash_key(secret_key)
    
    def modify_logits(self, logits, context):
        """Apply watermark to token logits."""
        # Your watermarking logic here
        watermark_mask = self.compute_mask(context)
        return logits + watermark_mask * self.strength
    
    def detect(self, text):
        """Detect watermark presence."""
        tokens = self.tokenize(text)
        score = self.compute_watermark_score(tokens)
        return score > self.threshold
```

## ğŸ” Detection

### Statistical Detection

```python
from watermark_lab.detection import StatisticalDetector

detector = StatisticalDetector(
    watermark_config=watermark.get_config(),
    test_type="multinomial"  # or "z_test", "chi_squared"
)

# Analyze text
analysis = detector.analyze(
    text=suspected_text,
    return_details=True
)

print(f"Test statistic: {analysis.test_statistic:.3f}")
print(f"P-value: {analysis.p_value:.6f}")
print(f"Token-level scores: {analysis.token_scores}")

# Visualize detection
detector.plot_token_probabilities(
    analysis,
    save_to="detection_analysis.png"
)
```

### Neural Detection

```python
from watermark_lab.detection import NeuralDetector

# Train neural watermark detector
detector = NeuralDetector(
    architecture="transformer",
    pretrained="roberta-base"
)

# Train on watermarked/clean pairs
detector.train(
    watermarked_texts=watermarked_data,
    clean_texts=clean_data,
    epochs=10
)

# Fast batch detection
predictions = detector.predict_batch(
    texts=test_texts,
    return_probabilities=True
)
```

### Multi-Watermark Detection

```python
from watermark_lab.detection import MultiWatermarkDetector

# Detect multiple watermark types
multi_detector = MultiWatermarkDetector()

# Register known watermarks
multi_detector.register("company_a", kirchenbauer_config)
multi_detector.register("company_b", markllm_config)
multi_detector.register("company_c", custom_config)

# Identify watermark source
result = multi_detector.identify_watermark(text)
print(f"Detected watermark: {result.watermark_id}")
print(f"Confidence: {result.confidence:.2%}")
```

## âš”ï¸ Attacks

### Paraphrase Attacks

```python
from watermark_lab.attacks import ParaphraseAttack

# Initialize paraphrase attacker
attacker = ParaphraseAttack(
    method="pegasus",  # or "t5", "chatgpt", "manual"
    strength="medium"   # or "light", "heavy"
)

# Attack watermarked text
attacked_text = attacker.attack(watermarked_text)

# Evaluate attack success
success = not detector.detect(attacked_text).is_watermarked
print(f"Attack successful: {success}")
print(f"Semantic similarity: {attacker.compute_similarity(watermarked_text, attacked_text):.2f}")
```

### Adversarial Attacks

```python
from watermark_lab.attacks import AdversarialAttack

# Gradient-based attack
adv_attack = AdversarialAttack(
    target_model=watermarker.model,
    attack_type="token_substitution",
    epsilon=0.1
)

# Generate adversarial examples
adversarial_text = adv_attack.generate(
    watermarked_text,
    target="remove_watermark",
    max_iterations=100
)

# Measure attack impact
impact = adv_attack.evaluate_impact(
    original=watermarked_text,
    adversarial=adversarial_text,
    metrics=["watermark_score", "perplexity", "semantic_similarity"]
)
```

### Robustness Evaluation

```python
from watermark_lab.attacks import RobustnessEvaluator

evaluator = RobustnessEvaluator()

# Test against multiple attacks
robustness_report = evaluator.evaluate(
    watermark_method="kirchenbauer",
    attacks=[
        "paraphrase_light",
        "paraphrase_heavy",
        "truncation",
        "synonym_replacement",
        "back_translation"
    ],
    num_samples=1000
)

# Generate robustness report
evaluator.generate_report(
    robustness_report,
    output="robustness_analysis.pdf"
)
```

## ğŸ“Š Evaluation

### Quality Metrics

```python
from watermark_lab.evaluation import QualityEvaluator

evaluator = QualityEvaluator()

# Compare watermarked vs non-watermarked
quality_metrics = evaluator.evaluate(
    original_texts=clean_texts,
    watermarked_texts=watermarked_texts,
    metrics=[
        "perplexity",
        "bleu",
        "rouge",
        "bertscore",
        "diversity",
        "coherence"
    ]
)

# Statistical significance testing
significance = evaluator.test_significance(
    quality_metrics,
    test="wilcoxon"
)
```

### Human Evaluation Proxy

```python
from watermark_lab.evaluation import HumanEvalProxy

# Simulate human evaluation
proxy = HumanEvalProxy(
    model="gpt-4",
    criteria=["fluency", "coherence", "naturalness"]
)

scores = proxy.evaluate(
    texts=watermarked_texts,
    reference_texts=clean_texts
)

print(f"Average human-like score: {scores.mean():.2f}/5")
```

### Trade-off Analysis

```python
from watermark_lab.evaluation import TradeoffAnalyzer

analyzer = TradeoffAnalyzer()

# Analyze detectability vs quality trade-off
tradeoff_curve = analyzer.compute_tradeoff(
    watermark_method="kirchenbauer",
    parameter_ranges={
        "gamma": [0.1, 0.25, 0.5],
        "delta": [0.5, 1.0, 2.0, 4.0]
    },
    metrics=["detection_rate", "perplexity_increase"]
)

# Find optimal parameters
optimal = analyzer.find_optimal_parameters(
    tradeoff_curve,
    weights={"detection": 0.7, "quality": 0.3}
)
```

## ğŸ”¬ Forensics

### Watermark Tracing

```python
from watermark_lab.forensics import WatermarkTracer

tracer = WatermarkTracer()

# Trace watermark through transformations
trace = tracer.trace_watermark(
    original=watermarked_text,
    transformed=modified_text,
    transformations=["paraphrase", "truncation", "translation"]
)

# Visualize watermark degradation
tracer.plot_signal_degradation(
    trace,
    save_to="watermark_trace.png"
)
```

### Contamination Analysis

```python
from watermark_lab.forensics import ContaminationAnalyzer

analyzer = ContaminationAnalyzer()

# Check for watermark contamination in dataset
contamination_report = analyzer.scan_dataset(
    dataset_path="data/training_corpus.txt",
    known_watermarks=[kirchenbauer_config, markllm_config],
    batch_size=1000
)

print(f"Contamination rate: {contamination_report.contamination_rate:.2%}")
print(f"Affected samples: {len(contamination_report.contaminated_samples)}")

# Clean contaminated data
clean_dataset = analyzer.remove_watermarked_samples(
    dataset_path,
    threshold=0.95
)
```

### Attribution

```python
from watermark_lab.forensics import WatermarkAttribution

# Identify text source
attributor = WatermarkAttribution(
    known_sources={
        "GPT-4": gpt4_watermark_config,
        "Claude": claude_watermark_config,
        "LLaMA": llama_watermark_config
    }
)

attribution = attributor.attribute_text(suspicious_text)
print(f"Most likely source: {attribution.source}")
print(f"Confidence: {attribution.confidence:.2%}")
```

## ğŸ“ˆ Benchmarks

### Standard Benchmark Suite

```python
from watermark_lab.benchmarks import StandardBenchmark

benchmark = StandardBenchmark()

# Run comprehensive evaluation
results = benchmark.run(
    watermark_methods=["all"],
    datasets=["c4", "openwebtext", "wikipedia"],
    attacks=["none", "light_paraphrase", "heavy_paraphrase"],
    metrics=["detection_rate", "perplexity", "attack_success_rate"]
)

# Generate leaderboard
benchmark.create_leaderboard(
    results,
    output_format="markdown",
    save_to="RESULTS.md"
)
```

### Performance Results

| Method | Detection Rate | Perplexity Increase | Paraphrase Robustness |
|--------|---------------|--------------------|-----------------------|
| Kirchenbauer | 99.5% | +0.15 | 75% |
| MarkLLM-KGW | 98.2% | +0.08 | 82% |
| Aaronson | 97.8% | +0.22 | 68% |
| Zhao | 96.5% | +0.10 | 79% |

### Computational Performance

```python
from watermark_lab.benchmarks import PerformanceBenchmark

perf_bench = PerformanceBenchmark()

# Measure generation and detection speed
perf_results = perf_bench.benchmark_speed(
    methods=["kirchenbauer", "markllm", "aaronson"],
    text_lengths=[100, 500, 1000, 5000],
    batch_sizes=[1, 8, 32]
)

perf_bench.plot_performance_curves(
    perf_results,
    metrics=["tokens_per_second", "detection_time_ms"]
)
```

## ğŸ¨ Visualization

### Interactive Dashboard

```python
from watermark_lab.visualization import WatermarkDashboard

# Launch interactive dashboard
dashboard = WatermarkDashboard()
dashboard.add_watermarker("kirchenbauer", kirchenbauer_watermark)
dashboard.add_watermarker("markllm", markllm_watermark)

dashboard.launch(
    port=8080,
    features=[
        "live_generation",
        "detection_playground",
        "attack_simulator",
        "quality_comparison"
    ]
)
```

### Analysis Plots

```python
from watermark_lab.visualization import WatermarkVisualizer

viz = WatermarkVisualizer()

# Token probability distribution
viz.plot_token_distribution(
    watermarked_text,
    watermark_config,
    save_to="token_dist.png"
)

# Watermark strength heatmap
viz.plot_watermark_heatmap(
    texts=[text1, text2, text3],
    detector=detector,
    save_to="watermark_heatmap.png"
)

# ROC curves for detection
viz.plot_roc_curves(
    methods=["kirchenbauer", "markllm", "aaronson"],
    test_set=test_data,
    save_to="roc_curves.png"
)
```

## ğŸ“š API Reference

### Core Classes

```python
class WatermarkFactory:
    @staticmethod
    def create(method: str, **kwargs) -> BaseWatermark
    
class BaseWatermark(ABC):
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str
    
    @abstractmethod
    def get_config(self) -> WatermarkConfig
    
class WatermarkDetector:
    def detect(self, text: str) -> DetectionResult
    def detect_batch(self, texts: List[str]) -> List[DetectionResult]
```

### Attack Interface

```python
class BaseAttack(ABC):
    @abstractmethod
    def attack(self, text: str) -> str
    
    def evaluate_success(self, original: str, attacked: str) -> bool
```

### Evaluation Metrics

```python
class MetricCalculator:
    def calculate_perplexity(self, text: str) -> float
    def calculate_detection_rate(self, texts: List[str]) -> float
    def calculate_robustness(self, method: str, attack: str) -> float
```

## ğŸ¤ Contributing

We welcome contributions! Priority areas:
- New watermarking algorithms
- Advanced attack methods
- Multilingual support
- Real-time detection optimization

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Clone repository
git clone https://github.com/your-org/lm-watermark-lab
cd lm-watermark-lab

# Install dev dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/

# Run benchmarks
python benchmarks/run_all.py
```

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Related Projects

- [Kirchenbauer Watermarking](https://github.com/jwkirchenbauer/lm-watermarking) - Original implementation
- [MarkLLM](https://github.com/THU-BPM/MarkLLM) - Comprehensive watermarking toolkit
- [Aaronson Watermarking](https://scottaaronson.blog/?p=6823) - Cryptographic approach
- [SWEET](https://github.com/microsoft/SWEET) - Microsoft's watermarking

## ğŸ“ Support

- ğŸ“§ Email: watermark@your-org.com
- ğŸ’¬ Discord: [Join our community](https://discord.gg/your-org)
- ğŸ“– Documentation: [Full docs](https://docs.your-org.com/watermark-lab)
- ğŸ“ Tutorial: [Watermarking Guide](https://learn.your-org.com/watermarking)

## ğŸ“š References

- [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226) - Kirchenbauer et al.
- [MarkLLM: An Open-Source Toolkit](https://github.com/THU-BPM/MarkLLM) - THU-BPM
- [Robust Multi-bit Watermarking](https://arxiv.org/abs/2307.15255) - Zhao et al.
- [On the Reliability of Watermarks](https://arxiv.org/abs/2306.04634) - Analysis paper
