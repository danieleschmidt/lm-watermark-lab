#!/usr/bin/env python3
"""
Comprehensive Quality Gates for Autonomous SDLC Implementation
Tests all three generations with comprehensive validation including security, performance, and production readiness.
"""

import sys
import os
import time
import subprocess
import hashlib
import json
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

def run_security_scan():
    """Run comprehensive security validation."""
    print("\nüîí Security Quality Gate")
    print("=" * 50)
    
    security_checks = {
        'input_sanitization': False,
        'rate_limiting': False,
        'authentication': False,
        'encryption': False,
        'audit_logging': False
    }
    
    try:
        # Check security module exists
        security_path = Path('src/watermark_lab/security')
        if security_path.exists():
            print("‚úÖ Security module structure exists")
            
            # Check individual security components
            security_files = {
                'input_sanitization.py': 'input_sanitization',
                'rate_limiting.py': 'rate_limiting',
                'authentication.py': 'authentication',
                'encryption.py': 'encryption',
                'audit_logging.py': 'audit_logging',
                '__init__.py': 'security_init'
            }
            
            for file_name, check_name in security_files.items():
                file_path = security_path / file_name
                if file_path.exists():
                    content = file_path.read_text()
                    
                    # Check for security patterns
                    security_patterns = [
                        'SecurityError', 'ValidationError', 'ThreatLevel',
                        'sanitize_text', 'detect_threats', 'rate_limit'
                    ]
                    
                    found_patterns = sum(1 for pattern in security_patterns if pattern in content)
                    if found_patterns >= 3:
                        security_checks[check_name if check_name in security_checks else 'input_sanitization'] = True
                        print(f"  ‚úÖ {file_name}: {found_patterns} security patterns found")
                    else:
                        print(f"  ‚ùå {file_name}: insufficient security patterns")
                else:
                    print(f"  ‚ùå {file_name}: missing")
        
        # Security score
        passed_checks = sum(security_checks.values())
        total_checks = len(security_checks)
        security_score = passed_checks / total_checks
        
        print(f"\nüîí Security Score: {security_score:.1%} ({passed_checks}/{total_checks} checks passed)")
        
        if security_score >= 0.4:  # At least basic security implemented
            print("‚úÖ Security quality gate PASSED")
            return True
        else:
            print("‚ùå Security quality gate FAILED")
            return False
            
    except Exception as e:
        print(f"‚ùå Security scan failed: {e}")
        return False

def run_performance_validation():
    """Run performance quality validation."""
    print("\n‚ö° Performance Quality Gate")
    print("=" * 50)
    
    performance_checks = {
        'caching_system': False,
        'optimization_module': False,
        'streaming_processor': False,
        'async_processing': False
    }
    
    try:
        # Check optimization module
        opt_path = Path('src/watermark_lab/optimization')
        if opt_path.exists():
            print("‚úÖ Optimization module exists")
            
            opt_files = {
                'caching.py': 'caching_system',
                '__init__.py': 'optimization_module'
            }
            
            for file_name, check_name in opt_files.items():
                file_path = opt_path / file_name
                if file_path.exists():
                    content = file_path.read_text()
                    
                    # Check for performance patterns
                    perf_patterns = [
                        'CacheManager', 'PerformanceOptimizer', 'async def',
                        'asyncio', 'threading', 'multiprocessing'
                    ]
                    
                    found_patterns = sum(1 for pattern in perf_patterns if pattern in content)
                    if found_patterns >= 2:
                        performance_checks[check_name] = True
                        print(f"  ‚úÖ {file_name}: {found_patterns} performance patterns found")
                    else:
                        print(f"  ‚ùå {file_name}: insufficient performance patterns")
        
        # Check streaming module
        stream_path = Path('src/watermark_lab/streaming')
        if stream_path.exists():
            print("‚úÖ Streaming module exists")
            
            stream_files = ['stream_processor.py', '__init__.py']
            for file_name in stream_files:
                file_path = stream_path / file_name
                if file_path.exists():
                    content = file_path.read_text()
                    
                    stream_patterns = [
                        'StreamProcessor', 'async def', 'asyncio',
                        'real_time', 'streaming'
                    ]
                    
                    found_patterns = sum(1 for pattern in stream_patterns if pattern in content)
                    if found_patterns >= 3:
                        performance_checks['streaming_processor'] = True
                        performance_checks['async_processing'] = True
                        print(f"  ‚úÖ {file_name}: {found_patterns} streaming patterns found")
        
        # Performance score
        passed_checks = sum(performance_checks.values())
        total_checks = len(performance_checks)
        perf_score = passed_checks / total_checks
        
        print(f"\n‚ö° Performance Score: {perf_score:.1%} ({passed_checks}/{total_checks} checks passed)")
        
        if perf_score >= 0.5:
            print("‚úÖ Performance quality gate PASSED")
            return True
        else:
            print("‚ùå Performance quality gate FAILED")
            return False
            
    except Exception as e:
        print(f"‚ùå Performance validation failed: {e}")
        return False

def run_scalability_validation():
    """Run scalability and production readiness validation."""
    print("\nüìà Scalability Quality Gate")
    print("=" * 50)
    
    scalability_checks = {
        'auto_scaling': False,
        'load_balancing': False,
        'monitoring': False,
        'deployment': False,
        'health_checks': False
    }
    
    try:
        # Check deployment module
        deploy_path = Path('src/watermark_lab/deployment')
        if deploy_path.exists():
            print("‚úÖ Deployment module exists")
            
            deploy_files = {
                'auto_scaler.py': 'auto_scaling',
                '__init__.py': 'deployment'
            }
            
            for file_name, check_name in deploy_files.items():
                file_path = deploy_path / file_name
                if file_path.exists():
                    content = file_path.read_text()
                    
                    scaling_patterns = [
                        'AutoScaler', 'ScalingConfig', 'scale_up', 'scale_down',
                        'MetricsPredictor', 'predictive'
                    ]
                    
                    found_patterns = sum(1 for pattern in scaling_patterns if pattern in content)
                    if found_patterns >= 4:
                        scalability_checks[check_name] = True
                        print(f"  ‚úÖ {file_name}: {found_patterns} scaling patterns found")
        
        # Check monitoring module
        monitor_path = Path('src/watermark_lab/monitoring')
        if monitor_path.exists():
            print("‚úÖ Monitoring module exists")
            
            monitor_files = {
                'health_monitor.py': 'monitoring',
                '__init__.py': 'health_checks'
            }
            
            for file_name, check_name in monitor_files.items():
                file_path = monitor_path / file_name
                if file_path.exists():
                    content = file_path.read_text()
                    
                    monitor_patterns = [
                        'HealthMonitor', 'HealthCheck', 'MetricsCollector',
                        'health_check', 'monitoring'
                    ]
                    
                    found_patterns = sum(1 for pattern in monitor_patterns if pattern in content)
                    if found_patterns >= 3:
                        scalability_checks[check_name] = True
                        print(f"  ‚úÖ {file_name}: {found_patterns} monitoring patterns found")
        
        # Check production files
        prod_files = {
            'docker-compose.yml': 'load_balancing',
            'kubernetes/deployment.yaml': 'deployment'
        }
        
        for file_path, check_name in prod_files.items():
            if Path(file_path).exists():
                scalability_checks[check_name] = True
                print(f"  ‚úÖ {file_path}: production config exists")
        
        # Scalability score
        passed_checks = sum(scalability_checks.values())
        total_checks = len(scalability_checks)
        scale_score = passed_checks / total_checks
        
        print(f"\nüìà Scalability Score: {scale_score:.1%} ({passed_checks}/{total_checks} checks passed)")
        
        if scale_score >= 0.6:
            print("‚úÖ Scalability quality gate PASSED")
            return True
        else:
            print("‚ùå Scalability quality gate FAILED")
            return False
            
    except Exception as e:
        print(f"‚ùå Scalability validation failed: {e}")
        return False

def run_research_validation():
    """Run research capabilities validation."""
    print("\nüî¨ Research Quality Gate") 
    print("=" * 50)
    
    research_checks = {
        'experimental_framework': False,
        'neural_training': False,
        'comparative_studies': False,
        'statistical_analysis': False,
        'reproducibility': False
    }
    
    try:
        # Check research module
        research_path = Path('src/watermark_lab/research')
        if research_path.exists():
            print("‚úÖ Research module exists")
            
            research_files = {
                'experimental_framework.py': 'experimental_framework',
                '__init__.py': 'reproducibility'
            }
            
            for file_name, check_name in research_files.items():
                file_path = research_path / file_name
                if file_path.exists():
                    content = file_path.read_text()
                    
                    research_patterns = [
                        'ExperimentalFramework', 'ExperimentConfig',
                        'statistical', 'reproducib', 'experiment'
                    ]
                    
                    found_patterns = sum(1 for pattern in research_patterns if pattern in content)
                    if found_patterns >= 3:
                        research_checks[check_name] = True
                        research_checks['statistical_analysis'] = True
                        print(f"  ‚úÖ {file_name}: {found_patterns} research patterns found")
        
        # Check neural module
        neural_path = Path('src/watermark_lab/neural')
        if neural_path.exists():
            print("‚úÖ Neural module exists")
            
            neural_files = ['trainer.py', '__init__.py']
            for file_name in neural_files:
                file_path = neural_path / file_name
                if file_path.exists():
                    content = file_path.read_text()
                    
                    neural_patterns = [
                        'NeuralTrainer', 'TrainingConfig', 'neural',
                        'training', 'torch', 'transformers'
                    ]
                    
                    found_patterns = sum(1 for pattern in neural_patterns if pattern in content)
                    if found_patterns >= 3:
                        research_checks['neural_training'] = True
                        research_checks['comparative_studies'] = True
                        print(f"  ‚úÖ {file_name}: {found_patterns} neural patterns found")
        
        # Research score
        passed_checks = sum(research_checks.values())
        total_checks = len(research_checks)
        research_score = passed_checks / total_checks
        
        print(f"\nüî¨ Research Score: {research_score:.1%} ({passed_checks}/{total_checks} checks passed)")
        
        if research_score >= 0.6:
            print("‚úÖ Research quality gate PASSED")
            return True
        else:
            print("‚ùå Research quality gate FAILED")  
            return False
            
    except Exception as e:
        print(f"‚ùå Research validation failed: {e}")
        return False

def run_code_quality_analysis():
    """Run comprehensive code quality analysis."""
    print("\nüìä Code Quality Analysis")
    print("=" * 50)
    
    try:
        # Count total files and lines
        src_path = Path('src')
        total_files = 0
        total_lines = 0
        documented_files = 0
        
        for py_file in src_path.rglob('*.py'):
            if '__pycache__' not in str(py_file):
                total_files += 1
                content = py_file.read_text()
                lines = len(content.splitlines())
                total_lines += lines
                
                # Check for documentation
                if '"""' in content or "'''" in content:
                    documented_files += 1
        
        print(f"üìÅ Total Python files: {total_files}")
        print(f"üìù Total lines of code: {total_lines:,}")
        print(f"üìñ Documentation coverage: {documented_files/max(1, total_files):.1%}")
        
        # Module complexity analysis
        modules = [
            'core', 'utils', 'api', 'cli', 'security',
            'optimization', 'streaming', 'deployment',
            'monitoring', 'research', 'neural', 'visualization'
        ]
        
        existing_modules = 0
        complex_modules = 0
        
        for module in modules:
            module_path = Path(f'src/watermark_lab/{module}')
            if module_path.exists():
                existing_modules += 1
                
                # Count files in module
                module_files = len(list(module_path.glob('*.py')))
                if module_files >= 2:  # Has multiple files (complex)
                    complex_modules += 1
                
                print(f"  üì¶ {module}: {module_files} files")
        
        print(f"\nüèóÔ∏è Architecture complexity: {existing_modules}/{len(modules)} modules ({existing_modules/len(modules):.1%})")
        print(f"üß© Complex modules: {complex_modules}/{existing_modules} ({complex_modules/max(1, existing_modules):.1%})")
        
        # Quality metrics
        quality_score = 0.0
        
        # Documentation quality (20%)
        doc_score = documented_files / max(1, total_files)
        quality_score += doc_score * 0.2
        
        # Architecture quality (30%)
        arch_score = existing_modules / len(modules)
        quality_score += arch_score * 0.3
        
        # Complexity quality (25%)
        complexity_score = complex_modules / max(1, existing_modules)
        quality_score += complexity_score * 0.25
        
        # Size quality (25%) - reasonable size indicates good structure
        size_score = min(1.0, total_lines / 15000)  # Optimal around 15k lines
        quality_score += size_score * 0.25
        
        print(f"\nüìä Overall Code Quality: {quality_score:.1%}")
        
        if quality_score >= 0.75:
            print("‚úÖ Code quality EXCELLENT")
            return True
        elif quality_score >= 0.6:
            print("‚úÖ Code quality GOOD")
            return True
        else:
            print("‚ö†Ô∏è Code quality needs improvement")
            return False
            
    except Exception as e:
        print(f"‚ùå Code quality analysis failed: {e}")
        return False

def run_integration_validation():
    """Test integration between all modules."""
    print("\nüîó Integration Quality Gate")
    print("=" * 50)
    
    integration_tests = {
        'core_factory': False,
        'api_endpoints': False,
        'cli_commands': False,
        'model_integration': False,
        'cache_integration': False
    }
    
    try:
        # Test core factory integration
        try:
            from watermark_lab.core.factory import WatermarkFactory
            methods = WatermarkFactory.list_methods()
            if len(methods) >= 3:  # At least 3 methods
                integration_tests['core_factory'] = True
                print(f"  ‚úÖ Core factory: {len(methods)} methods available")
            else:
                print(f"  ‚ùå Core factory: only {len(methods)} methods")
        except Exception as e:
            print(f"  ‚ùå Core factory import failed: {e}")
        
        # Test API structure
        api_files = ['main.py', '__init__.py']
        api_path = Path('src/watermark_lab/api')
        
        if api_path.exists() and all((api_path / f).exists() for f in api_files):
            integration_tests['api_endpoints'] = True
            print("  ‚úÖ API structure complete")
        else:
            print("  ‚ùå API structure incomplete")
        
        # Test CLI structure
        cli_files = ['main.py', '__init__.py']
        cli_path = Path('src/watermark_lab/cli')
        
        if cli_path.exists() and all((cli_path / f).exists() for f in cli_files):
            integration_tests['cli_commands'] = True
            print("  ‚úÖ CLI structure complete")
        else:
            print("  ‚ùå CLI structure incomplete")
        
        # Test model integration
        try:
            model_loader_path = Path('src/watermark_lab/utils/model_loader.py')
            if model_loader_path.exists():
                content = model_loader_path.read_text()
                if 'ModelManager' in content and 'TransformersModelWrapper' in content:
                    integration_tests['model_integration'] = True
                    print("  ‚úÖ Model integration available")
                else:
                    print("  ‚ùå Model integration incomplete")
            else:
                print("  ‚ùå Model loader missing")
        except Exception as e:
            print(f"  ‚ùå Model integration test failed: {e}")
        
        # Test cache integration
        try:
            cache_path = Path('src/watermark_lab/optimization/caching.py')
            if cache_path.exists():
                content = cache_path.read_text()
                if 'CacheManager' in content and 'get_cache_manager' in content:
                    integration_tests['cache_integration'] = True
                    print("  ‚úÖ Cache integration available")
                else:
                    print("  ‚ùå Cache integration incomplete")
        except Exception as e:
            print(f"  ‚ùå Cache integration test failed: {e}")
        
        # Integration score
        passed_tests = sum(integration_tests.values())
        total_tests = len(integration_tests)
        integration_score = passed_tests / total_tests
        
        print(f"\nüîó Integration Score: {integration_score:.1%} ({passed_tests}/{total_tests} tests passed)")
        
        if integration_score >= 0.8:
            print("‚úÖ Integration quality gate PASSED")
            return True
        else:
            print("‚ùå Integration quality gate FAILED")
            return False
            
    except Exception as e:
        print(f"‚ùå Integration validation failed: {e}")
        return False

def run_production_readiness_check():
    """Check production readiness."""
    print("\nüöÄ Production Readiness Gate")
    print("=" * 50)
    
    prod_checks = {
        'docker_config': False,
        'kubernetes_config': False,
        'monitoring_config': False,
        'environment_config': False,
        'security_config': False
    }
    
    try:
        # Docker configuration
        docker_files = ['Dockerfile', 'docker-compose.yml']
        docker_count = sum(1 for f in docker_files if Path(f).exists())
        if docker_count >= 1:
            prod_checks['docker_config'] = True
            print(f"  ‚úÖ Docker: {docker_count}/2 config files present")
        else:
            print("  ‚ùå Docker: no configuration files")
        
        # Kubernetes configuration
        k8s_path = Path('kubernetes')
        if k8s_path.exists():
            k8s_files = list(k8s_path.glob('*.yaml'))
            if len(k8s_files) >= 1:
                prod_checks['kubernetes_config'] = True
                print(f"  ‚úÖ Kubernetes: {len(k8s_files)} config files")
            else:
                print("  ‚ùå Kubernetes: no YAML configs")
        else:
            print("  ‚ùå Kubernetes: directory missing")
        
        # Monitoring configuration
        monitoring_path = Path('monitoring')
        if monitoring_path.exists():
            monitor_files = list(monitoring_path.glob('*.yml'))
            if len(monitor_files) >= 2:
                prod_checks['monitoring_config'] = True
                print(f"  ‚úÖ Monitoring: {len(monitor_files)} config files")
            else:
                print(f"  ‚ùå Monitoring: only {len(monitor_files)} config files")
        else:
            print("  ‚ùå Monitoring: directory missing")
        
        # Environment configuration
        env_files = ['pyproject.toml', '.env.example', 'requirements.md']
        env_count = sum(1 for f in env_files if Path(f).exists())
        if env_count >= 2:
            prod_checks['environment_config'] = True
            print(f"  ‚úÖ Environment: {env_count}/3 config files present")
        else:
            print(f"  ‚ùå Environment: only {env_count}/3 config files")
        
        # Security configuration
        sec_files = ['SECURITY.md', 'src/watermark_lab/security/__init__.py']
        sec_count = sum(1 for f in sec_files if Path(f).exists())
        if sec_count >= 1:
            prod_checks['security_config'] = True
            print(f"  ‚úÖ Security: {sec_count}/2 security files present")
        else:
            print("  ‚ùå Security: no security configuration")
        
        # Production score
        passed_checks = sum(prod_checks.values())
        total_checks = len(prod_checks)
        prod_score = passed_checks / total_checks
        
        print(f"\nüöÄ Production Readiness: {prod_score:.1%} ({passed_checks}/{total_checks} checks passed)")
        
        if prod_score >= 0.8:
            print("‚úÖ Production readiness gate PASSED")
            return True
        else:
            print("‚ùå Production readiness gate FAILED")
            return False
            
    except Exception as e:
        print(f"‚ùå Production readiness check failed: {e}")
        return False

def generate_quality_report():
    """Generate comprehensive quality report."""
    print("\nüìã Generating Quality Report...")
    
    # Run all quality gates
    quality_results = {
        'security': run_security_scan(),
        'performance': run_performance_validation(), 
        'scalability': run_scalability_validation(),
        'research': run_research_validation(),
        'code_quality': run_code_quality_analysis(),
        'integration': run_integration_validation(),
        'production': run_production_readiness_check()
    }
    
    # Calculate overall score
    passed_gates = sum(quality_results.values())
    total_gates = len(quality_results)
    overall_score = passed_gates / total_gates
    
    # Generate report
    report = {
        'timestamp': time.time(),
        'overall_score': overall_score,
        'passed_gates': passed_gates,
        'total_gates': total_gates,
        'quality_results': quality_results,
        'grade': 'A' if overall_score >= 0.9 else 'B' if overall_score >= 0.8 else 'C' if overall_score >= 0.7 else 'D' if overall_score >= 0.6 else 'F'
    }
    
    # Save report
    report_path = Path('quality_report.json')
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    return report

def main():
    """Run comprehensive quality gates validation."""
    
    print("üéØ AUTONOMOUS SDLC - QUALITY GATES VALIDATION")
    print("=" * 80)
    print("Comprehensive validation of all three generations:")
    print("  ‚Ä¢ Generation 1: Basic functionality with ML models")
    print("  ‚Ä¢ Generation 2: Robustness with security & neural training")
    print("  ‚Ä¢ Generation 3: Production scaling & optimization")
    print("=" * 80)
    
    start_time = time.time()
    
    try:
        # Generate comprehensive quality report
        report = generate_quality_report()
        
        # Final summary
        print(f"\n{'='*20} FINAL QUALITY REPORT {'='*20}")
        print(f"üìä Overall Quality Score: {report['overall_score']:.1%}")
        print(f"üéØ Quality Grade: {report['grade']}")
        print(f"‚úÖ Passed Gates: {report['passed_gates']}/{report['total_gates']}")
        
        print(f"\nüìã Individual Gate Results:")
        for gate_name, passed in report['quality_results'].items():
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            print(f"  {status} {gate_name.replace('_', ' ').title()}")
        
        duration = time.time() - start_time
        print(f"\n‚è±Ô∏è Total validation time: {duration:.2f} seconds")
        print(f"üìÑ Report saved to: quality_report.json")
        
        # Final verdict
        if report['overall_score'] >= 0.85:
            print(f"\nüèÜ AUTONOMOUS SDLC IMPLEMENTATION: EXCELLENT")
            print("üöÄ Ready for production deployment!")
            print("‚ú® All quality gates demonstrate production-grade implementation")
            success = True
        elif report['overall_score'] >= 0.75:
            print(f"\nüéâ AUTONOMOUS SDLC IMPLEMENTATION: GOOD")
            print("‚úÖ Ready for staging deployment")
            print("‚ö†Ô∏è Some quality gates need minor improvements")
            success = True
        elif report['overall_score'] >= 0.6:
            print(f"\n‚ö†Ô∏è AUTONOMOUS SDLC IMPLEMENTATION: ACCEPTABLE")
            print("üîß Needs improvements before production")
            print("‚ùå Several quality gates require attention")
            success = False
        else:
            print(f"\n‚ùå AUTONOMOUS SDLC IMPLEMENTATION: NEEDS WORK")
            print("üö´ Not ready for deployment")
            print("üî® Significant improvements required")
            success = False
        
        return success
        
    except Exception as e:
        print(f"\nüí• Quality gates validation failed: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)